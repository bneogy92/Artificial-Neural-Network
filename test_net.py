from os import listdir
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from numpy import *
import pickle


def main():

    root_path = os.path.dirname(os.path.realpath(__file__))
    test_path = os.path.join(root_path,'data','kaggle_test_data.csv')
    weight_path = os.path.join(root_path,'weights.txt')
    #print(weight_path)

    #Reading Test data from csv
    test_data = pd.read_csv(test_path)
    test_id = test_data['id']

    


    #Cleansing Categorical attributes of Test data
    test = test_data.drop(['id','education'],axis = 1)
    test_categorical = test.select_dtypes(include=['object']).copy()
    test_categorical.head()
    test_categorical["sex"]= np.where(test_categorical["sex"].str.contains("Male"), 1, 0)
    test_categorical = pd.get_dummies(test_categorical, columns=["workclass","marital-status","occupation","relationship","race","native-country"])
    test_integer = test.select_dtypes(include=['int64']).copy()

    #Normalizing numerical attributes of test dataset
    
    test_integer['age'] = normalize_data(test_integer['age'])
    test_integer['fnlwgt'] = normalize_data(test_integer['fnlwgt'])
    test_integer['education-num'] = normalize_data(test_integer['education-num'])
    test_integer['capital-gain'] = normalize_data(test_integer['capital-gain'])
    test_integer['capital-loss'] = normalize_data(test_integer['capital-loss'])
    test_integer['hours-per-week'] = normalize_data(test_integer['hours-per-week'])
    test_new = np.concatenate((test_categorical, test_integer), axis=1)


    #Principal Component Analysis for Feature visualization and dimensionality reduction
    #Mean Vectors
    mean_test = calculate_mean(test_new)
    scatter_test = scatter(test_new,mean_test)

    #Eigen Value and Eigen Vector
    eig_value_test,eig_vect_test = np.linalg.eig(scatter_test)
    #show_eig(eig_value_train,eig_vect_train,train_new)
    #show_eig(eig_value_test,eig_vect_test,test_new)

    #Eigen Vectors
    #visualize_eig(eig_value_train)
    visualize_eig(eig_value_test)

    #Feature data after selecting 25 principal components
    feature_test = feature_data(eig_value_test,eig_vect_test)

    #Transform to get new subspace
    test_final = test_new.dot(feature_test.T)


    #Prediction Results
    model_test = readDict(weight_path)
    y_pred = predict(model_test,test_final)
    print('Predicted class :',y_pred)
    write_output_to_csv(y_pred,test_id)



#Generating eigen value,eigen vector pairs
def feature_data(eig_value,eig_vect):
    eigen_pairs = [(np.abs(eig_value[i]),eig_vect[i,:]) for i in range(len(eig_value))]
    eigen_pairs.sort(key = lambda x:x[0],reverse = True)

    feature_data = []
    for i in range(0,43):
        feature_data.append(eigen_pairs[i][1])
    feature_data = np.array(feature_data)
    return feature_data



#Plot to visualize eigen values
def visualize_eig(eig_value):
    fig = plt.figure(figsize=(10,5))
    sing_vals = np.arange(len(eig_value))+1
    plt.plot(sing_vals,eig_value,linewidth =2)
    plt.xlabel('PC')
    plt.ylabel('Eigen Value')
    plt.ylim(ymax = max(eig_value), ymin = min(eig_value))
    plt.title("Plot")
    plt.show()



#Generating Covariance matrix
def scatter(phi,mean_vector):
    scatter_matrix = np.zeros((np.shape(phi)[1],np.shape(phi)[1]))
    for i in range(np.shape(phi)[0]):
        scatter_matrix += (phi[i,:].reshape(phi.shape[1],1)-mean_vector).dot((phi[i,:].reshape(phi.shape[1],1)-mean_vector).T)
    return scatter_matrix


#Calculating feature mean vectors
def calculate_mean(phi):
    mean_vector = np.zeros(np.shape(phi))
    mean_vector = np.mean(phi,axis=0)
    return mean_vector
    
    
#Normalizing numerical features
def normalize_data(data):
    mean = np.mean(data)
    stdev = np.std(data)
    normalized_data = np.divide(np.subtract(data,mean),stdev)
    return normalized_data



#Function to predict an output
def predict(model,phi):
    W1,b1,W2,b2 = model['W1'],model['b1'],model['W2'],model['b2']
    #Forward propagation
    print(np.shape(phi))
    print(np.shape(W1))
    z1 = phi.dot(W1)+b1
    a1 = tanh(z1)
    z2 = a1.dot(W2)+b2
    exp_scores = np.exp(z2)
    probs = exp_scores/np.sum(exp_scores,axis=1,keepdims=True) #Softmax function at output layer
    return np.argmax(probs,axis=1)


#Reading weight vector generated by training
def readDict(filename):
    with open(filename,"rb") as handle:
        dict= pickle.loads(handle.read())
    return dict


#Writing classification output to csv
def write_output_to_csv(x,test_id):
    heading_1 = np.array("id")
    a = test_id.reshape(np.shape(x)[0],1)
    heading_2 = np.array("salary")
    b = np.vstack((heading_1,a))
    c = np.vstack((heading_2,x.reshape(np.shape(x)[0],1)))
    out = np.hstack((b,c))
    np.savetxt("predictions.csv",out,fmt="%s,%s",delimiter=',')
    




if __name__ == '__main__':
    main()


